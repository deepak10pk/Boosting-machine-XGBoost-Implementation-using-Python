# Boosting-machine-XGBoost-Implementation-using-Python
* In my previous Repositories we saw models based on bagging; random forests and extraTrees where each individual model was independent and eventual prediction of the ensemble of these models was a simple majority vote or average depending on whether the problem was of classiﬁcation of regression.
* The randomness in the process, helped the model become less aﬀected by noise and more generalizable. However this bagging did not really help in underlying models to become better at extracting more complex patterns
* Boosting machines go that extra step, in modern implementation, both the ideas; using randomness to make models generalizable and boosting are used.  We can consider boosting machines to be more powerful than bagging models . However that comes with downside of them being prone to over-ﬁtting in theory. We'll learn about Extreme Gradient Boosting (Xgboost) which goes one step further and adds the element of regularization to boosting machines and some other radical changes to become one of the most successful Machine Learning Algorithms in the recent history.
* Just like bagging , boosting machines also are made up of multiple individual models . However in boosting machines , individual models are built sequentially and eventual model is summation of individual models not the average . 

# Case Study
>>We will consider the demographic data 'census_income.csv' for this module. This is typical census data. This data has been labeled with annual income less than 50K dollars or not. We want to build a model such that given these census characteristics we can ﬁgure out if someone will fall in a category in which their income is higher than 50K dollars or not. Such models are mainly used when formulating government policies.
